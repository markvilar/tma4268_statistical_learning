# here you write your code
ks = 1:30
train.e = rep(NA,length(ks))
test.e = rep(NA,length(ks))
train = M[tr,-1]
test = M[-tr,-1]
train_labels = M[tr,1]
test_labels = M[-tr,1]
# misclassification error = # of misclassifications / # of samples
for (k in ks){
y_hat = class::knn(train=train, cl=train_labels, test=test, k=k, prob=TRUE)
test.e[k] = mean(test_labels != y_hat)
test.e[k] = mean(trains_labels != y_hat)
}
# here you write your code
ks = 1:30
train.e = rep(NA,length(ks))
test.e = rep(NA,length(ks))
train = M[tr,-1]
test = M[-tr,-1]
train_labels = M[tr,1]
test_labels = M[-tr,1]
# misclassification error = # of misclassifications / # of samples
for (k in ks){
y_hat = class::knn(train=train, cl=train_labels, test=test, k=k, prob=TRUE)
test.e[k] = mean(test_labels != y_hat)
train.e[k] = mean(train_labels != y_hat)
}
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
# here you write your code
ks = 1:30
train.e = rep(NA,length(ks))
test.e = rep(NA,length(ks))
train = M[tr,-1]
test = M[-tr,-1]
train_labels = M[tr,1]
test_labels = M[-tr,1]
for (k in ks){
y_hat = class::knn(train=train, cl=train_labels, test=test, k=k, prob=TRUE)
test.e[k] = mean(test_labels != y_hat)
y_hat$prob
}
# here you write your code
ks = 1:30
train.e = rep(NA,length(ks))
test.e = rep(NA,length(ks))
train = M[tr,-1]
test = M[-tr,-1]
train_labels = M[tr,1]
test_labels = M[-tr,1]
for (k in ks){
y_hat = class::knn(train=train, cl=train_labels, test=test, k=k, prob=TRUE)
test.e[k] = mean(test_labels != y_hat)
y_hat.prob
}
# here you write your code
ks = 1:30
train.e = rep(NA,length(ks))
test.e = rep(NA,length(ks))
train = M[tr,-1]
test = M[-tr,-1]
train_labels = M[tr,1]
test_labels = M[-tr,1]
for (k in ks){
y_hat = class::knn(train=train, cl=train_labels, test=test, k=k, prob=TRUE)
test.e[k] = mean(test_labels != y_hat)
}
table(test_labels, y_hat)
# here you write your code
ks = 1:30
train.e = rep(NA,length(ks))
test.e = rep(NA,length(ks))
train = M[tr,-1]
test = M[-tr,-1]
train_labels = M[tr,1]
test_labels = M[-tr,1]
for (k in ks){
y_hat_test = class::knn(train=train, cl=train_labels, test=test, k=k)
y_hat_train = class::knn(train=train, cl=train_labels, test=train, k=k)
test.e[k] = mean(test_labels != y_hat_test)
train.e[k] = mean(train_labels != y_hat_train)
}
table(test_labels, y_hat)
# here you write your code
ks = 1:30
train.e = rep(NA,length(ks))
test.e = rep(NA,length(ks))
train = M[tr,-1]
test = M[-tr,-1]
train_labels = M[tr,1]
test_labels = M[-tr,1]
for (k in ks){
y_hat_test = class::knn(train=train, cl=train_labels, test=test, k=k)
y_hat_train = class::knn(train=train, cl=train_labels, test=train, k=k)
test.e[k] = mean(test_labels != y_hat_test)
train.e[k] = mean(train_labels != y_hat_train)
}
length(y_hat_test)
length(y_hat_train)
# here you write your code
ks = 1:30
train.e = rep(NA,length(ks))
test.e = rep(NA,length(ks))
train = M[tr,-1]
test = M[-tr,-1]
train_labels = M[tr,1]
test_labels = M[-tr,1]
for (k in ks){
y_hat_test = class::knn(train=train, cl=train_labels, test=test, k=k)
y_hat_train = class::knn(train=train, cl=train_labels, test=train, k=k)
test.e[k] = mean(test_labels != y_hat_test)
train.e[k] = mean(train_labels != y_hat_train)
}
length(y_hat_test)
length(y_hat_train)
length(test_labels)
length(train_labels)
# here you write your code
ks = 1:30
train.e = rep(NA,length(ks))
test.e = rep(NA,length(ks))
train = M[tr,-1]
test = M[-tr,-1]
train_labels = M[tr,1]
test_labels = M[-tr,1]
for (k in ks){
y_hat_test = class::knn(train=train, cl=train_labels, test=test, k=k)
y_hat_train = class::knn(train=train, cl=train_labels, test=train, k=k)
test.e[k] = mean(test_labels != y_hat_test)
train.e[k] = mean(train_labels != y_hat_train)
}
set.seed(0)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
View(find_internal_minimas_idxs)
View(cv)
View(raw)
cv.e = mean(cv)
cv.se = #fill in
k.min = # fill in
cv.e = mean(cv)
#cv.se = #fill in
#k.min = # fill in
cv.e = mean(cv[,:])
cv.e = mean(cv[,])
#cv.se = #fill in
#k.min = # fill in
cv.e = colMeans(cv)
#cv.se = #fill in
#k.min = # fill in
View(cv)
View(cv)
cv.e = colMeans(cv)
n_fols = dims(cv)
cv.e = colMeans(cv)
n_fols = dim(cv)
#cv.se =
#k.min = # fill in
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = colSds(cv)
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = apply(cv, 2, sd)
#k.min = # fill in
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = (1/sqrt(n_folds)) * apply(cv, 2, sd)
#k.min = # fill in
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = apply(cv, 2, sd)
#k.min = # fill in
set.seed(0)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = (1/sqrt(n_folds)) * apply(cv, 2, sd)
#k.min = # fill in
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = (1/sqrt(n_folds)) * apply(cv, 2, sd)
cv_e_se_sum = cv.e + cv.se
#k.min = # fill in
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = (1/sqrt(n_folds)) * apply(cv, 2, sd)
k.min = which.min(cv.e + cv.se)
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = (1/sqrt(n_folds)) * apply(cv, 2, sd)
k.min = which.min(cv.e + cv.se)
k.min
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = (1/sqrt(n_folds)) * apply(cv, 2, sd)
k.min = which.min(cv.e)
k.min
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = (1/sqrt(n_folds)) * apply(cv, 2, sd)
k.min = which.min(cv.se)
k.min
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = (1/sqrt(n_folds)) * apply(cv, 2, sd)
k.min = which.min(cv.e + cv.se)
k.min
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
cv.e = colMeans(cv)
n_folds = dim(cv)[1]
cv.se = (1/sqrt(n_folds)) * apply(cv, 2, sd)
k.min = which.min(cv.e)
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size = 100
xnew = apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(M[tr,-1], M[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5,
xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(M[1:np,-1], col=factor(M[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"),
col=c("red", "black"), pch=1)
box()
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size = 100
xnew = apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(M[tr,-1], M[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5,
xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(M[1:np,-1], col=factor(M[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"),
col=c("red", "black"), pch=1)
box()
K=30
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set
library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1]
# in your call to the function roc in the pROC library
roc(predictor=KNNprob, response=M[-tr,1])
K=30
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set
library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1]
# in your call to the function roc in the pROC library
pROC::roc(predictor=KNNprob, response=M[-tr,1])
?roc
K=30
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set
library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1]
# in your call to the function roc in the pROC library
pROC::roc(response=M[-tr,1], predictor=KNNprob)
K=30
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set
library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1]
# in your call to the function roc in the pROC library
pROC::roc(response=M[-tr,1], predictor=KNNprob, plot=TRUE)
K=30
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set
library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1]
# in your call to the function roc in the pROC library
my_roc = roc(response=M[-tr,1], predictor=KNNprob, plot=TRUE)
K=30
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set
library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1]
# in your call to the function roc in the pROC library
my_roc = roc(response=M[-tr,1], predictor=KNNprob)
?plot
K=30
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k=K, prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob = ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set
library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1]
# in your call to the function roc in the pROC library
my_roc = roc(response=M[-tr,1], predictor=KNNprob)
plot(my_roc$specificities, my_roc$sensitivities, type="l", xlim=c(1.0, 0), ylim=c(0, 1.0), xlab = "Specificity", ylab="Sensitivity")
View(train)
length(train)
height(train)
dim(train)
View(t)
y_bar = function(x){
n_vars = dim(x)[2]
n_samps = dim(x)[1]
y_bar = rep(NA,n_samps)
for(i in 1:n_samps){
y_bar[i] = which.max(x[i,])
}
y_bar = factor(y_bar)
return(y_bar)
}
y_bar_train = y_bar(train)
length(y_bar_train)
y_bar = function(x){
n_vars = dim(x)[2]
n_samps = dim(x)[1]
y_bar = rep(NA,n_samps)
for(i in 1:n_samps){
y_bar[i] = which.max(x[i,])
}
y_bar = factor(y_bar)
return(y_bar)
}
y_bar_train = y_bar(train)
y_bar_test = y_bar(test)
length(y_bar_test)
View(train)
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size = 100
xnew = apply(M[tr,-1], 2, y_bar(X) seq(min(X), max(X), length.out=size))
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size = 100
xnew = apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(M[tr,-1], M[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5,
xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(M[1:np,-1], col=factor(M[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"),
col=c("red", "black"), pch=1)
box()
y_bar = function(x){
n_vars = dim(x)[2]
n_samps = dim(x)[1]
y_bar = rep(NA,n_samps)
for(i in 1:n_samps){
y_bar[i] = which.max(x[i,])
}
y_bar = factor(y_bar)
return(y_bar)
}
y_bar_test = y_bar(test)
y_bar_error = mean(test_labels != y_bar_test)
y_bar = function(x){
n_vars = dim(x)[2]
n_samps = dim(x)[1]
y_bar = rep(NA,n_samps)
for(i in 1:n_samps){
y_bar[i] = which.max(x[i,])
}
y_bar = factor(y_bar)
return(y_bar)
}
y_bar_test = y_bar(test)
y_bar_error = mean(test_labels != y_bar_test)
length(test_labels)
length(y_bar_test)
y_bar = function(x){
n_vars = dim(x)[2]
n_samps = dim(x)[1]
y_bar = rep(NA,n_samps)
for(i in 1:n_samps){
max_idx = which.max(x[i,])
y_bar[i] = max_idx - 1
}
y_bar = factor(y_bar)
return(y_bar)
}
y_bar_test = y_bar(test)
y_bar_error = mean(test_labels != y_bar_test)
y_hat_error = test.e[K]
y_bar = function(x){
n_vars = dim(x)[2]
n_samps = dim(x)[1]
y_bar = rep(NA,n_samps)
for(i in 1:n_samps){
max_idx = which.max(x[i,])
y_bar[i] = max_idx - 1
}
y_bar = factor(y_bar)
return(y_bar)
}
y_bar_test = y_bar(test)
y_bar_error = mean(test_labels != y_bar_test)
y_hat_error = test.e[K]
confusionMatrix(y_bar_test)
y_bar = function(x){
n_vars = dim(x)[2]
n_samps = dim(x)[1]
y_bar = rep(NA,n_samps)
for(i in 1:n_samps){
max_idx = which.max(x[i,])
y_bar[i] = max_idx - 1
}
y_bar = factor(y_bar)
return(y_bar)
}
y_bar_test = y_bar(test)
y_bar_error = mean(test_labels != y_bar_test)
y_hat_error = test.e[K]
confusionMatrix(y_bar_test, test_labels)
y_bar = function(x){
n_vars = dim(x)[2]
n_samps = dim(x)[1]
y_bar = rep(NA,n_samps)
for(i in 1:n_samps){
max_idx = which.max(x[i,])
y_bar[i] = max_idx - 1
}
y_bar = factor(y_bar)
return(y_bar)
}
y_bar_test = y_bar(test)
y_hat_test = class::knn(train=train, cl=train_labels, test=test, k=K)
y_bar_error = mean(test_labels != y_bar_test)
y_hat_error = mean(test_labels != y_hat_test)
confusionMatrix(y_bar_test, test_labels)
confusionMatrix(y_hat_test, test_labels)
